"""
============================================================
PHASE 2.5: Rule Learning Agent
============================================================
Responsibility:
    Convert validated LLM findings into deterministic rules
    compatible with CodeAnalysisAgent. Persists rules to disk.

Input:  LLMFinding + MCPValidation
Output: Persisted rule in rules/learned_rules.py

Design Notes:
    - Generated rules are pure functions with no side effects
    - Uses rule signature hashing to prevent duplicates
    - Rules follow the (line_number, explanation, confidence)
      return interface of existing rules
    - Human-readable and auditable
============================================================
"""

import os
import hashlib
import logging
import re
from datetime import datetime
from typing import Optional

from models.data_models import LLMFinding, MCPValidation
import config

logger = logging.getLogger(__name__)


class RuleLearningAgent:
    """
    Agent: Rule Learning
    Converts validated LLM findings into persistent
    deterministic rules for future runs.
    """

    def __init__(self):
        self.rules_dir = config.LEARNED_RULES_DIR
        self.rules_file = os.path.join(self.rules_dir, "learned_rules.py")
        self._ensure_rules_dir()
        self._existing_hashes = self._load_existing_hashes()
        logger.info(
            f"RuleLearningAgent initialized. "
            f"Rules dir: {self.rules_dir}, "
            f"Existing rules: {len(self._existing_hashes)}"
        )

    def _ensure_rules_dir(self):
        """Create the rules directory and files if they don't exist."""
        os.makedirs(self.rules_dir, exist_ok=True)

        # Create __init__.py if missing
        init_file = os.path.join(self.rules_dir, "__init__.py")
        if not os.path.exists(init_file):
            with open(init_file, "w", encoding="utf-8") as f:
                f.write('"""Learned rules package."""\n')

        # Create learned_rules.py if missing
        if not os.path.exists(self.rules_file):
            with open(self.rules_file, "w", encoding="utf-8") as f:
                f.write(self._get_file_header())

    def _get_file_header(self) -> str:
        """Return the header for learned_rules.py."""
        return '''"""
============================================================
Learned Rules — Auto-Generated by RuleLearningAgent
============================================================
These rules were discovered by the LLM fallback engine and
validated against MCP documentation. They follow the same
interface as deterministic rules in CodeAnalysisAgent:

    def rule_name(lines, code, sample) -> Optional[Tuple[int, str, float]]
        Returns (line_number, explanation, confidence) or None

DO NOT EDIT MANUALLY unless you understand the implications.
============================================================
"""

import re
from typing import List, Optional, Tuple

# Rule registry — maps rule_name to function
LEARNED_RULES = {}


def register_rule(name):
    """Decorator to register a learned rule."""
    def decorator(fn):
        LEARNED_RULES[name] = fn
        return fn
    return decorator


# ── Learned Rules Below ────────────────────────────────────

'''

    def learn(
        self,
        finding: LLMFinding,
        validation: MCPValidation,
    ) -> Optional[str]:
        """
        Convert a validated LLM finding into a deterministic rule.

        Args:
            finding: Validated LLM finding
            validation: MCP validation result

        Returns:
            Rule name if successfully learned, None otherwise
        """
        if not finding.suggested_rule:
            logger.info("  No suggested rule in LLM finding, skipping.")
            return None

        rule = finding.suggested_rule
        rule_hash = self._compute_hash(rule.rule_name, rule.detection_pattern)

        # ── Check for duplicates ───────────────────────────
        if rule_hash in self._existing_hashes:
            logger.info(
                f"  Rule '{rule.rule_name}' already exists (hash={rule_hash[:8]}), "
                f"skipping."
            )
            return None

        # ── Generate the rule function ─────────────────────
        rule_code = self._generate_rule_code(finding, validation, rule_hash)

        # ── Append to learned_rules.py ─────────────────────
        try:
            with open(self.rules_file, "a", encoding="utf-8") as f:
                f.write(rule_code)
            self._existing_hashes.add(rule_hash)
            logger.info(
                f"  ✓ Learned new rule: '{rule.rule_name}' "
                f"(severity={rule.severity}, hash={rule_hash[:8]})"
            )
            return rule.rule_name
        except Exception as e:
            logger.error(f"  Failed to persist rule: {e}")
            return None

    def _generate_rule_code(
        self,
        finding: LLMFinding,
        validation: MCPValidation,
        rule_hash: str,
    ) -> str:
        """Generate Python code for the learned rule."""
        rule = finding.suggested_rule
        safe_name = re.sub(r'[^a-z0-9_]', '_', rule.rule_name.lower())
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Escape strings for embedding in Python code
        desc = rule.description.replace('"', '\\"').replace("\n", " ")
        pattern = rule.detection_pattern.replace('"', '\\"').replace("\n", " ")
        reasoning = finding.reasoning.replace('"', '\\"').replace("\n", " ")[:200]
        bug_type = finding.bug_type.replace('"', '\\"')

        # Build MCP citation
        mcp_cite = ""
        if validation.supporting_docs:
            first_doc = validation.supporting_docs[0][:100].replace('"', '\\"').replace("\n", " ")
            mcp_cite = f" According to documentation: {first_doc}"

        code = f'''
# ── Rule: {safe_name} ──────────────────────────────────────
# Hash: {rule_hash}
# Learned: {timestamp}
# Severity: {rule.severity}
# Source: LLM (Groq OSS-120B) + MCP Validation
# Description: {desc}
@register_rule("{safe_name}")
def _check_{safe_name}(lines, code, sample):
    """
    {desc}
    Detection: {pattern}
    """
    try:
        pattern = re.compile(r"""{pattern}""", re.IGNORECASE)
        for line_obj in lines:
            if pattern.search(line_obj.stripped):
                explanation = (
                    f"Line {{line_obj.line_number}}: {bug_type} — {desc}"
                    f" |{mcp_cite}"
                    f" | Impact: Potential API misuse detected."
                )
                return (line_obj.line_number, explanation, {finding.confidence:.2f})
    except re.error:
        # If the pattern is not valid regex, use substring matching
        search_term = """{pattern}""".lower()
        for line_obj in lines:
            if search_term in line_obj.stripped.lower():
                explanation = (
                    f"Line {{line_obj.line_number}}: {bug_type} — {desc}"
                    f" |{mcp_cite}"
                    f" | Impact: Potential API misuse detected."
                )
                return (line_obj.line_number, explanation, {finding.confidence:.2f})
    return None

'''
        return code

    def _compute_hash(self, rule_name: str, detection_pattern: str) -> str:
        """Compute a unique hash for a rule to detect duplicates."""
        sig = f"{rule_name}::{detection_pattern}"
        return hashlib.sha256(sig.encode()).hexdigest()[:16]

    def _load_existing_hashes(self) -> set:
        """Load existing rule hashes from the learned_rules.py file."""
        hashes = set()
        if not os.path.exists(self.rules_file):
            return hashes

        try:
            with open(self.rules_file, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip().startswith("# Hash:"):
                        h = line.strip().replace("# Hash:", "").strip()
                        if h:
                            hashes.add(h)
        except Exception:
            pass

        return hashes
